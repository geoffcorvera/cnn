{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "catdog.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1DTMD1xpRXsLgB9pYFumEUIvo009xgsNJ",
      "authorship_tag": "ABX9TyO5OnIOY4rcZu3f5BJiKX92",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geoffcorvera/cnn/blob/colab/catdog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxo6jC0KtwUm"
      },
      "source": [
        "import os\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental import preprocessing as preprocessing_layers\n",
        "from tensorflow.math import confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k27Ev1ArFi_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41bdbe58-b593-4dae-e6a8-1f5c63377c40"
      },
      "source": [
        "# Confirm GPU connection\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print(f'Found GPU at: {device_name}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ESCOtfvM_z"
      },
      "source": [
        "# Load Pre-Trained Model\n",
        "Import InceptionResNetV2 pre-trained model - without the final classifying labels. We will append our own binary classifier, by flattening the output of InceptionResNetV2, then adding a fully connected layer and final output layer with a single neuron with sigmoid output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE9KxubI-Fvg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd42a22f-9e3d-4ae4-a9c5-0c28d8626434"
      },
      "source": [
        "# Load resnet pretrained on ImageNet\n",
        "pre_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
        "pre_model.trainable = False\n",
        "\n",
        "save_dir = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision'\n",
        "file_name = 'InceptionResNetV2_summary.txt'\n",
        "f_path = os.path.join(save_dir, file_name)\n",
        "with open(f_path, 'w') as f:\n",
        "  pre_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "  f.close()\n",
        "\n",
        "# Plot model to file\n",
        "# plot_img_file = os.path.join(save_dir, 'pre_model_plot.jpg')\n",
        "# tf.keras.utils.plot_model(pre_model, to_file=plot_img_file, show_shapes=True) "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "ZxoMrp7As3ze",
        "outputId": "64e77957-4b29-499d-bda3-e5e52d7aba57"
      },
      "source": [
        "# Visualize inception resnet first layer filters\n",
        "def getConvLayer(model):\n",
        "  res = None\n",
        "  for layer in model.layers:\n",
        "    if 'conv' in layer.name:\n",
        "      res = layer\n",
        "      break\n",
        "  return res\n",
        "\n",
        "def normalize(X):\n",
        "  return (X - np.min(X))/(np.max(X) - np.min(X))\n",
        "\n",
        "def visualizeFilters(filters):\n",
        "  nf = filters.shape[3]\n",
        "  fig, _ = plt.subplots(4,8)\n",
        "  \n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    f = filters[:,:,:,i]\n",
        "    f = normalize(f)\n",
        "    ax.imshow(f, cmap='gray')\n",
        "    ax.set_axis_off()\n",
        "\n",
        "  fig.suptitle('1st Convolution Layer Filters')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "conv_layer = getConvLayer(pre_model)\n",
        "visualizeFilters(conv_layer.get_weights()[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 32 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ-YWDMxULTD"
      },
      "source": [
        "# Build Model\n",
        "A binary classifier is added to the pre-trained model. The model also specifies preprocessing so it can take raw image data. Data is augmented with horizontal flips, random rotation and zooming to improve generalizability/prevent overfitting to the training data. Another layer rescales pixel values within the range [0,1].\n",
        "\n",
        "The model is compiled with binary cross entropy loss and RMSprop optimizer, and 'accuracy' metrics are enabled for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQMCbqpc0KWt",
        "outputId": "5182dd6f-b8d3-4979-832a-5ed0177b4c26"
      },
      "source": [
        "def binary_img_classifier(base_model):\n",
        "\n",
        "  data_augmentation = keras.Sequential(\n",
        "      [\n",
        "        preprocessing_layers.RandomFlip('horizontal'),\n",
        "        preprocessing_layers.RandomRotation(0.1),\n",
        "        preprocessing_layers.RandomZoom(0.1),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  input_shape = (150,150,3)\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  x = data_augmentation(inputs)           #Augment data to increase training set\n",
        "  x = preprocessing_layers.Rescaling(1.0/255)(x) #Rescale image values to [0,1]\n",
        "\n",
        "  outputs = base_model(x)\n",
        "  outputs = layers.Flatten()(outputs) #flatten output of pre_model\n",
        "  outputs = layers.Dense(256, activation='relu')(outputs) #adds dense layer w/ 256 neurons + ReLU activation\n",
        "  outputs = layers.Dense(1, activation='sigmoid')(outputs) #output layer w/ single neuron, output in range [0,1]\n",
        "\n",
        "  model = keras.Model(inputs, outputs)\n",
        "\n",
        "  model.compile(optimizer='rmsprop',\n",
        "                metrics=['accuracy'],\n",
        "                loss='binary_crossentropy')\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "model = binary_img_classifier(pre_model)\n",
        "model.summary()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_7 (Sequential)    (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "rescaling_4 (Rescaling)      (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "inception_resnet_v2 (Functio (None, 3, 3, 1536)        54336736  \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 13824)             0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               3539200   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 57,876,193\n",
            "Trainable params: 3,539,457\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aFL1heJg554"
      },
      "source": [
        "# Evaluate transfer model before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMEPLGlJg4BY"
      },
      "source": [
        "def get_preds_labels(model, dat):\n",
        "  truths = []\n",
        "  preds = []\n",
        "  for X,Y in dat:\n",
        "    truths.append(Y.numpy().astype(int))\n",
        "    preds.append(model.predict(X))\n",
        "\n",
        "  preds = np.concatenate(preds, axis=0)\n",
        "  truths = np.concatenate(truths, axis=0)\n",
        "  # Threshold at 0.9\n",
        "  preds = np.where(preds>0.9,1,0).astype(int)\n",
        "  return (preds, truths)\n",
        "\n",
        "\n",
        "def show_conf_mat(y_pred, y_true, class_names):\n",
        "  con_mat = confusion_matrix(labels=y_true, predictions=y_pred).numpy()\n",
        "  \n",
        "  fig, ax = plt.subplots()\n",
        "  ax.matshow(con_mat, cmap='cividis')\n",
        "  for (i, j), z in np.ndenumerate(con_mat):\n",
        "    ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n",
        "  ax.set_xticklabels(['']+class_names)\n",
        "  ax.set_yticklabels(['']+class_names)\n",
        "  plt.show()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "Hxz5wHQ7hvYY",
        "outputId": "17f3c0d2-c952-4033-980d-33aa325e1271"
      },
      "source": [
        "model.evaluate(test_ds)\n",
        "y_pred, y_true = get_preds_labels(model, test_ds)\n",
        "show_conf_mat(y_pred.reshape(-1), y_true.reshape(-1), test_ds.class_names)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 8s 222ms/step - loss: 0.9028 - accuracy: 0.5017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy3uwUUrrQwY"
      },
      "source": [
        "When running the built-in evaluate function, the model was able to achieve 50% accuracy against the test set of cats & dogs before training. From the confusion matrix, we can see that the pre-trained InceptionResNetV2 model can propertly identify almost 60% of the cat examples, and 74% of the dog examples. InceptionResNetV2 was pre-trained on ImageNet, a dataset which contains examples of cats and dogs.\n",
        "\n",
        "Initially, the network is miscategorizing a significant amount of cats as dogs. In the following steps, we will fine-tune the network on more images of cats & dogs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVWxjozXQyl6"
      },
      "source": [
        "# Import Data\n",
        "All images are resized to (150x150). The training dataset is split into training & validation datasets. Most of the preprocessing is specified within the model (see above). Note: Rescaling pixel intensities is done within the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhw3dducvMmr",
        "outputId": "bb0870bb-55e4-45ef-a1cf-ceb531339889"
      },
      "source": [
        "import tensorflow.keras.preprocessing as preprocessing\n",
        "from google.colab import drive\n",
        "\n",
        "print('Importing training and test datasets...\\n')\n",
        "batch_size = 64\n",
        "\n",
        "train_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/p3_data/cats_dogs_dataset/dataset/training_set'\n",
        "all_ds = preprocessing.image_dataset_from_directory(train_path,\n",
        "                                                    image_size=(150,150),\n",
        "                                                    shuffle=True,\n",
        "                                                    label_mode='binary',\n",
        "                                                    batch_size=batch_size)\n",
        "\n",
        "val_size = 12\n",
        "val_ds = all_ds.take(val_size)\n",
        "train_ds = all_ds.skip(val_size)\n",
        "\n",
        "test_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/p3_data/cats_dogs_dataset/dataset/test_set'\n",
        "test_ds = preprocessing.image_dataset_from_directory(test_path,\n",
        "                                                     image_size=(150,150),\n",
        "                                                     label_mode='binary',\n",
        "                                                     batch_size=batch_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing training and test datasets...\n",
            "\n",
            "Found 8282 files belonging to 2 classes.\n",
            "Found 2003 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpOLl6TbEeAY"
      },
      "source": [
        "# Train\n",
        "The model is compiled with RMSprop optimizer and binary cross-entropy loss function.\n",
        "\n",
        "Training continues for 5 epochs, with 12 batches from the training dataset designated for validation during fitting.\n",
        "\n",
        "Weights are saved periodically during training to the \"training\" directory. When model fitting is complete, the model is saved to \"last_trained\" directory for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN5eMw5EPVzW"
      },
      "source": [
        "# Optional: Load previously trained model & SKIP NEXT 2 CODE BLOCKS\n",
        "model = keras.models.load_model(r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/saved_models/v4_catdog_binaryaccuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1cahjH-_POA"
      },
      "source": [
        "# fit model & save weights during training & after\n",
        "def fit_to_data(model, train_ds, val_ds):\n",
        "  checkpoint_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training'\n",
        "  checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                  save_weights_only=True,\n",
        "                                                  verbose=1)\n",
        "\n",
        "  history = model.fit(train_ds,\n",
        "                      epochs=5,\n",
        "                      callbacks=[cp_callback],\n",
        "                      validation_data=val_ds)\n",
        "\n",
        "  return history"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72Wq1f-mfJuA",
        "outputId": "15a408ac-1eca-45cd-c3fc-0de448788f3e"
      },
      "source": [
        "train_hist_full_model = fit_to_data(model, train_ds, val_ds)\n",
        "# Save trained model\n",
        "save_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/last_trained'\n",
        "model.save(save_path)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "118/118 [==============================] - 46s 290ms/step - loss: 2.7002 - accuracy: 0.9002 - val_loss: 0.2511 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 2/5\n",
            "118/118 [==============================] - 37s 293ms/step - loss: 0.7210 - accuracy: 0.9236 - val_loss: 0.6088 - val_accuracy: 0.9440\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 3/5\n",
            "118/118 [==============================] - 37s 294ms/step - loss: 0.4308 - accuracy: 0.9352 - val_loss: 0.7284 - val_accuracy: 0.8724\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 4/5\n",
            "118/118 [==============================] - 38s 297ms/step - loss: 0.3747 - accuracy: 0.9381 - val_loss: 0.2033 - val_accuracy: 0.9518\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 5/5\n",
            "118/118 [==============================] - 38s 295ms/step - loss: 0.2587 - accuracy: 0.9421 - val_loss: 0.0947 - val_accuracy: 0.9701\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/last_trained/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OayS_KR9tid4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "ce115a4d-2d4a-49a8-f76b-9bb74d984787"
      },
      "source": [
        "results = model.evaluate(test_ds)\n",
        "\n",
        "preds, truths = get_preds_labels(model, test_ds)\n",
        "show_conf_mat(preds.reshape(-1), truths.reshape(-1), test_ds.class_names)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 8s 222ms/step - loss: 0.0815 - accuracy: 0.9750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogVnokVxvg1n"
      },
      "source": [
        "After fitting the model to the training set of cats & dogs, the built-in evaluate function reports an accuracy of 97% against the test set. During training, the reported per-epoch loss against the validation set was: 0.2511, 0.6088, 0.7284, 0.2033, 0.0947. Epochs 2 & 3 saw greater loss and lower accuracy against the validation set - a sign that the model was overfitting to training data. However, performance against the validation set improved in subsequent epochs; with accuracy against the validation set reaching 97% at epoch 5.\n",
        "\n",
        "The final confusion matrix of predictions against the test set demonstrates that the network was successfully fine-tuned to cats & dogs after training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY8fY1s_cqb5"
      },
      "source": [
        "# Create Sub-Network\n",
        "The first 18 layers of the InceptionResNetV2 model are taken to create a new, smaller network. It's outputs are flattened, fed into a fully-connected layer with 256 neurons and ReLU activation. The final output layer has a single neuron with sigmoid activation.\n",
        "\n",
        "The summary of the layers pulled from InceptionResNetV2 is shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96qeksmaWomZ",
        "outputId": "c966d48e-ba49-407d-9a04-4630a4eef6b1"
      },
      "source": [
        "def get_subnet(original, nlayers):\n",
        "  sub = keras.Sequential()\n",
        "\n",
        "  for i,l in enumerate(pre_model.layers):\n",
        "    if i > nlayers:\n",
        "      break\n",
        "    sub.add(l)\n",
        "  \n",
        "  return sub\n",
        "\n",
        "sub_net = get_subnet(pre_model, 18)\n",
        "sub_net.summary()\n",
        "\n",
        "# Transfer head to sub-network\n",
        "sub_net = binary_img_classifier(sub_net)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 74, 74, 32)        864       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 74, 74, 32)        96        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 72, 72, 32)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 72, 72, 32)        96        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 72, 72, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18432     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 72, 72, 64)        192       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 72, 72, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 35, 35, 80)        5120      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 35, 35, 80)        240       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 35, 35, 80)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 33, 33, 192)       138240    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 33, 33, 192)       576       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 33, 33, 192)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 16, 16, 64)        12288     \n",
            "=================================================================\n",
            "Total params: 185,360\n",
            "Trainable params: 0\n",
            "Non-trainable params: 185,360\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZXDgKUQpKZl"
      },
      "source": [
        "# Train & Evaluate Sub-Network\n",
        "The new network is fit to the training data. During training, accuracy & loss against the training and validation sets are shown (averaged across each epoch). After 5 epochs, accuracy against the validation set increased by 6%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYjMTWz-adNr",
        "outputId": "979b55f5-508e-46f8-e3f1-c741e10f4a16"
      },
      "source": [
        "history_subnet_train = fit_to_data(sub_net, train_ds, val_ds)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting to data...\n",
            "\n",
            "Epoch 1/5\n",
            "118/118 [==============================] - 1672s 12s/step - loss: 7.9370 - accuracy: 0.5653 - val_loss: 0.5694 - val_accuracy: 0.7083\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 2/5\n",
            "118/118 [==============================] - 25s 189ms/step - loss: 0.6582 - accuracy: 0.6894 - val_loss: 0.5964 - val_accuracy: 0.7148\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 3/5\n",
            "118/118 [==============================] - 25s 187ms/step - loss: 0.5959 - accuracy: 0.7050 - val_loss: 0.4873 - val_accuracy: 0.7669\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 4/5\n",
            "118/118 [==============================] - 25s 187ms/step - loss: 0.5420 - accuracy: 0.7418 - val_loss: 0.6679 - val_accuracy: 0.7005\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 5/5\n",
            "118/118 [==============================] - 25s 189ms/step - loss: 0.5345 - accuracy: 0.7441 - val_loss: 0.4681 - val_accuracy: 0.7682\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/last_trained/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "wjYe5t1WiCGA",
        "outputId": "82d59e85-9710-4762-b547-4bad0ce72c5a"
      },
      "source": [
        "preds, truths = get_preds_labels(sub_net, test_ds)\n",
        "show_conf_mat(preds.reshape(-1), truths.reshape(-1), test_ds.class_names)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2003, 1) (2003, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKkAHja6oGU5",
        "outputId": "f74eba08-13ea-4b14-91ce-813b3e8c1761"
      },
      "source": [
        "sub_net.evaluate(test_ds)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 5s 128ms/step - loss: 1.1314 - accuracy: 0.4951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1395853757858276, 0.4942586123943329]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPDvbRNAxK_z"
      },
      "source": [
        "The shallower network was not able to perform as well as the deeper network with 5 epochs. The network was only able to achieve 74% accuracy against the training, and 76% against the validation set during training. The final accuracy agains the test set reported by the evaluate function is 49%. The confusion matrix for the test set predictions show an overall accuracy of 70% - appearing to perform only slightly better than the deeper, untrained network. The shallower network may require more training epochs to achieve better performance, and appears to be unable to generalize as well as its deeper counterpart."
      ]
    }
  ]
}