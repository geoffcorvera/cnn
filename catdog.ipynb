{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "catdog.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1DTMD1xpRXsLgB9pYFumEUIvo009xgsNJ",
      "authorship_tag": "ABX9TyOO/YTZTPUHonoQ6RFmVmQb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geoffcorvera/cnn/blob/colab/catdog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ESCOtfvM_z"
      },
      "source": [
        "Build model on top of CNN pre-trained on ImageNet dataset. The final classification layer contains a single neuron with output in range [0,1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxo6jC0KtwUm"
      },
      "source": [
        "import os\n",
        "import numpy as np \n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from keras import models\n",
        "from keras import layers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k27Ev1ArFi_B",
        "outputId": "970b75be-4b4b-41ab-fa72-160ddc695b82"
      },
      "source": [
        "# Confirm TPU connection\n",
        "import tensorflow as tf \n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print(f'Found GPU at: {device_name}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE9KxubI-Fvg",
        "outputId": "06d4e62d-94d4-4060-840c-d50e0e56629e"
      },
      "source": [
        "# Load resnet pretrained on ImageNet\n",
        "pre_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
        "pre_model.trainable = False"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "ZxoMrp7As3ze",
        "outputId": "38e0ba6f-e6d0-4772-9059-1dda6c1abd34"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize inception resnet first layer filters\n",
        "def getConvLayer(model):\n",
        "  res = None\n",
        "  for layer in model.layers:\n",
        "    if 'conv' in layer.name:\n",
        "      res = layer\n",
        "      break\n",
        "  return res\n",
        "\n",
        "def visualizeFilters(filters):\n",
        "  # filters = filters[0]\n",
        "  nf = filters.shape[3]\n",
        "  fig, _ = plt.subplots(4,8)\n",
        "  \n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    ax.imshow(filters[:,:,:,i])\n",
        "    ax.set_axis_off()\n",
        "  fig.suptitle('1st Convolution Layer Filters')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "conv_layer = getConvLayer(pre_model)\n",
        "filters = conv_layer.get_weights()\n",
        "visualizeFilters(conv_layer.get_weights()[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 32 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQMCbqpc0KWt",
        "outputId": "a8898b9e-39fe-401c-c97b-b0bb69759e76"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental import preprocessing as preprocessing_layers\n",
        "\n",
        "print('Building model...\\n')\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      preprocessing_layers.RandomFlip('horizontal'),\n",
        "      preprocessing_layers.RandomRotation(0.1),\n",
        "      preprocessing_layers.RandomZoom(0.1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "input_shape = (150,150,3)\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = data_augmentation(inputs)           #Augment data to increase training set\n",
        "x = preprocessing_layers.Rescaling(1.0/255)(x) #Rescale image values to [0,1]\n",
        "\n",
        "outputs = pre_model(x)\n",
        "outputs = layers.Flatten()(outputs)\n",
        "outputs = layers.Dense(256, activation='relu')(outputs)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(outputs)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building model...\n",
            "\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "rescaling (Rescaling)        (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "inception_resnet_v2 (Functio (None, 3, 3, 1536)        54336736  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 13824)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               3539200   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 57,876,193\n",
            "Trainable params: 3,539,457\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhw3dducvMmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251358fd-3afc-4f30-b8f9-b8d799c4c2be"
      },
      "source": [
        "import tensorflow.keras.preprocessing as preprocessing\n",
        "from google.colab import drive\n",
        "\n",
        "print('Importing training and test datasets...\\n')\n",
        "training_dir = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/p3_data/cats_dogs_dataset.zip (Unzipped Files)/dataset/training_set'\n",
        "train_ds = preprocessing.image_dataset_from_directory(training_dir, image_size=(150,150))\n",
        "# train_ds = train_ds.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "test_dir = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/p3_data/cats_dogs_dataset.zip (Unzipped Files)/dataset/test_set'\n",
        "test_ds = preprocessing.image_dataset_from_directory(test_dir, image_size=(150,150))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing training and test datasets...\n",
            "\n",
            "Found 8000 files belonging to 2 classes.\n",
            "Found 2000 files belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdeHbYSEVSCw"
      },
      "source": [
        "!pip install pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1cahjH-_POA",
        "outputId": "90076bf4-b182-41b4-f5ae-806d2e0aa4ac"
      },
      "source": [
        "checkpoint_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
        "print('Fitting to data...\\n')\n",
        "history = model.fit(train_ds, epochs=3, callbacks=[cp_callback])\n",
        "\n",
        "os.listdir(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting to data...\n",
            "\n",
            "Epoch 1/3\n",
            " 74/250 [=======>......................] - ETA: 22:20 - loss: 10.3093"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeDGEWGkQpN3",
        "outputId": "43b7d613-ed68-414e-b166-3142f500cdd4"
      },
      "source": [
        "print('Evaluating against test...\\n')\n",
        "results = model.evaluate(test_ds)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate...\n",
            "\n",
            "63/63 [==============================] - 10s 141ms/step - loss: 225.6516\n",
            "225.651611328125\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}