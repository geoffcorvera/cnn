{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "catdog.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1DTMD1xpRXsLgB9pYFumEUIvo009xgsNJ",
      "authorship_tag": "ABX9TyPMM0mdjaVDdWVVzWJz9dET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geoffcorvera/cnn/blob/colab/catdog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ESCOtfvM_z"
      },
      "source": [
        "Build model on top of CNN pre-trained on ImageNet dataset. The final classification layer contains a single neuron with output in range [0,1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxo6jC0KtwUm"
      },
      "source": [
        "import os\n",
        "import numpy as np \n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from keras import models\n",
        "from keras import layers"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k27Ev1ArFi_B",
        "outputId": "970b75be-4b4b-41ab-fa72-160ddc695b82"
      },
      "source": [
        "# Confirm TPU connection\n",
        "import tensorflow as tf \n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print(f'Found GPU at: {device_name}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE9KxubI-Fvg",
        "outputId": "06d4e62d-94d4-4060-840c-d50e0e56629e"
      },
      "source": [
        "# Load resnet pretrained on ImageNet\n",
        "pre_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
        "pre_model.trainable = False"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoMrp7As3ze"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize inception resnet first layer filters\n",
        "def getConvLayer(model):\n",
        "  res = None\n",
        "  for layer in model.layers:\n",
        "    if 'conv' in layer.name:\n",
        "      res = layer\n",
        "      break\n",
        "  return res\n",
        "\n",
        "def visualizeFilters(filters):\n",
        "  # filters = filters[0]\n",
        "  nf = filters.shape[3]\n",
        "  fig, _ = plt.subplots(4,8)\n",
        "  \n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    ax.imshow(filters[:,:,:,i])\n",
        "    ax.set_axis_off()\n",
        "  fig.suptitle('1st Convolution Layer Filters')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "conv_layer = getConvLayer(pre_model)\n",
        "filters = conv_layer.get_weights()\n",
        "visualizeFilters(conv_layer.get_weights()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQMCbqpc0KWt",
        "outputId": "a8898b9e-39fe-401c-c97b-b0bb69759e76"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental import preprocessing as preprocessing_layers\n",
        "\n",
        "print('Building model...\\n')\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      preprocessing_layers.RandomFlip('horizontal'),\n",
        "      preprocessing_layers.RandomRotation(0.1),\n",
        "      preprocessing_layers.RandomZoom(0.1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "input_shape = (150,150,3)\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = data_augmentation(inputs)           #Augment data to increase training set\n",
        "x = preprocessing_layers.Rescaling(1.0/255)(x) #Rescale image values to [0,1]\n",
        "\n",
        "outputs = pre_model(x)\n",
        "outputs = layers.Flatten()(outputs)\n",
        "outputs = layers.Dense(256, activation='relu')(outputs)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(outputs)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building model...\n",
            "\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "rescaling (Rescaling)        (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "inception_resnet_v2 (Functio (None, 3, 3, 1536)        54336736  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 13824)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               3539200   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 57,876,193\n",
            "Trainable params: 3,539,457\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhw3dducvMmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34fadc2a-eafa-4fec-e4bd-df259463f615"
      },
      "source": [
        "import tensorflow.keras.preprocessing as preprocessing\n",
        "from google.colab import drive\n",
        "\n",
        "print('Importing training and test datasets...\\n')\n",
        "batch_size = 64\n",
        "val_size = batch_size*20\n",
        "\n",
        "train_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/p3_data/cats_dogs_dataset/dataset/training_set'\n",
        "all_ds = preprocessing.image_dataset_from_directory(train_path, image_size=(150,150), batch_size=batch_size)\n",
        "val_ds = all_ds.take(val_size)\n",
        "train_ds = all_ds.skip(val_size)\n",
        "# train_ds = train_ds.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "test_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/p3_data/cats_dogs_dataset/dataset/test_set (1)'\n",
        "test_ds = preprocessing.image_dataset_from_directory(test_path, image_size=(150,150), batch_size=batch_size)\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing training and test datasets...\n",
            "\n",
            "Found 8282 files belonging to 2 classes.\n",
            "Found 2003 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpOLl6TbEeAY"
      },
      "source": [
        "# Train\n",
        "The model is compiled with RMSprop optimizer and binary cross-entropy loss function.\n",
        "\n",
        "Training continues for 3 epochs, with 0.2% of the training set used for validation to check generalization.\n",
        "\n",
        "Weights are saved periodically during training to the \"training\" directory. When model fitting is complete, the model is saved to \"last_trained\" directory for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1cahjH-_POA",
        "outputId": "8c366348-c6d4-4663-c7f1-05946319b84c"
      },
      "source": [
        "checkpoint_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              metrics=['binary_accuracy']\n",
        "              loss='binary_crossentropy')\n",
        "print('Fitting to data...\\n')\n",
        "history = model.fit(train_ds,\n",
        "                    epochs=5,\n",
        "                    steps_per_epoch=1500,\n",
        "                    callbacks=[cp_callback],\n",
        "                    validation_data=val_ds)\n",
        "\n",
        "os.listdir(checkpoint_dir)\n",
        "\n",
        "save_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/last_trained'\n",
        "model.save(save_path)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting to data...\n",
            "\n",
            "Epoch 1/5\n",
            " 130/1500 [=>............................] - ETA: 5:53 - loss: 0.1180WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 7500 batches). You may need to use the repeat() function when building your dataset.\n",
            "1500/1500 [==============================] - 44s 22ms/step - loss: 0.1030\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/last_trained/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeDGEWGkQpN3",
        "outputId": "53dd6ca2-74e3-4a3d-b1cb-406d259e5b67"
      },
      "source": [
        "print('Evaluating against test...\\n')\n",
        "results = model.evaluate(test_ds)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating against test...\n",
            "\n",
            "32/32 [==============================] - 12s 242ms/step - loss: 0.0780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IosjrCQadOXN"
      },
      "source": [
        "y_true = np.concatenate([y.numpy() for _,y in test_ds])\n",
        "y_pred = model.predict(test_ds).reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olVP4q7QLVma",
        "outputId": "3c8064c6-a620-4e66-f327-1428339e8308"
      },
      "source": [
        "threshold = np.where(y_pred>0.9, 1, 0).astype(int) #threshold at 0.9\n",
        "assert y_pred.shape == y_true.shape\n",
        "\n",
        "correct = 0\n",
        "for i, inst in enumerate(zip(threshold, y_true)):\n",
        "  if i % 200 == 0:\n",
        "    print(f\"{i} => predicted: {inst[0]}\\tactual: {inst[1]}\")\n",
        "  if inst[0] == inst[1]:\n",
        "    correct += 1\n",
        "\n",
        "print(f'Accuracy = {correct/len(y_pred)}%')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 => predicted: 0\tactual: 0\n",
            "200 => predicted: 0\tactual: 1\n",
            "400 => predicted: 0\tactual: 0\n",
            "600 => predicted: 0\tactual: 1\n",
            "800 => predicted: 0\tactual: 1\n",
            "1000 => predicted: 0\tactual: 1\n",
            "1200 => predicted: 0\tactual: 1\n",
            "1400 => predicted: 0\tactual: 0\n",
            "1600 => predicted: 0\tactual: 0\n",
            "1800 => predicted: 1\tactual: 1\n",
            "2000 => predicted: 0\tactual: 0\n",
            "Accuracy = 0.5117324013979031%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "rVotnjQfjKlh",
        "outputId": "25c43be2-60f7-4f4a-e443-9c5e4061a751"
      },
      "source": [
        "from tensorflow.math import confusion_matrix\n",
        "\n",
        "con_mat = confusion_matrix(labels=y_true, predictions=threshold).numpy()\n",
        "print(con_mat)\n",
        "\n",
        "plt.matshow(con_mat)\n",
        "plt.show()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[515 485]\n",
            " [493 510]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}