{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "catdog.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1DTMD1xpRXsLgB9pYFumEUIvo009xgsNJ",
      "authorship_tag": "ABX9TyOVDakhzzrNTkFzd/9k5pLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geoffcorvera/cnn/blob/colab/catdog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxo6jC0KtwUm"
      },
      "source": [
        "import os\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental import preprocessing as preprocessing_layers\n",
        "from tensorflow.math import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k27Ev1ArFi_B"
      },
      "source": [
        "# Confirm GPU connection\n",
        "import tensorflow as tf \n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print(f'Found GPU at: {device_name}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ESCOtfvM_z"
      },
      "source": [
        "# Load Pre-Trained Model\n",
        "Import InceptionResNetV2 pre-trained model - without the final classifying labels. We will append our own binary classifier, by flattening the output of InceptionResNetV2, then adding a fully connected layer and final output layer with a single neuron with sigmoid output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE9KxubI-Fvg",
        "outputId": "ab2173c8-7bc4-4c84-c778-29cfcf681759"
      },
      "source": [
        "# Load resnet pretrained on ImageNet\n",
        "pre_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
        "pre_model.trainable = False"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "ZxoMrp7As3ze",
        "outputId": "644d29c4-db7c-4f57-864e-7e47b8cfdb58"
      },
      "source": [
        "# Visualize inception resnet first layer filters\n",
        "def getConvLayer(model):\n",
        "  res = None\n",
        "  for layer in model.layers:\n",
        "    if 'conv' in layer.name:\n",
        "      res = layer\n",
        "      break\n",
        "  return res\n",
        "\n",
        "def normalize(X):\n",
        "  return (X - np.min(X))/(np.max(X) - np.min(X))\n",
        "\n",
        "def visualizeFilters(filters):\n",
        "  nf = filters.shape[3]\n",
        "  fig, _ = plt.subplots(4,8)\n",
        "  \n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    f = filters[:,:,:,i]\n",
        "    f = normalize(f)\n",
        "    ax.imshow(f, cmap='gray')\n",
        "    ax.set_axis_off()\n",
        "\n",
        "  fig.suptitle('1st Convolution Layer Filters')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "conv_layer = getConvLayer(pre_model)\n",
        "visualizeFilters(conv_layer.get_weights()[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 32 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ-YWDMxULTD"
      },
      "source": [
        "# Build Model\n",
        "A binary classifier is added to the pre-trained model. The model also specifies preprocessing so it can take raw image data. Data is augmented with horizontal flips, random rotation and zooming. Pixel values are rescaled to range [0,1].\n",
        "\n",
        "The model is compiled with binary cross entropy loss and RMSprop optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQMCbqpc0KWt",
        "outputId": "21834ce4-f357-4064-d610-719359751daf"
      },
      "source": [
        "print('Building model...\\n')\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      preprocessing_layers.RandomFlip('horizontal'),\n",
        "      preprocessing_layers.RandomRotation(0.1),\n",
        "      preprocessing_layers.RandomZoom(0.1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "input_shape = (150,150,3)\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = data_augmentation(inputs)           #Augment data to increase training set\n",
        "x = preprocessing_layers.Rescaling(1.0/255)(x) #Rescale image values to [0,1]\n",
        "\n",
        "outputs = pre_model(x)\n",
        "outputs = layers.Flatten()(outputs) #flatten output of pre_model\n",
        "outputs = layers.Dense(256, activation='relu')(outputs) #adds dense layer w/ 256 neurons + ReLU activation\n",
        "outputs = layers.Dense(1, activation='sigmoid')(outputs) #output layer w/ single neuron, output in range [0,1]\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              metrics=['accuracy'],\n",
        "              loss='binary_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building model...\n",
            "\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "rescaling (Rescaling)        (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "inception_resnet_v2 (Functio (None, 3, 3, 1536)        54336736  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 13824)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               3539200   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 57,876,193\n",
            "Trainable params: 3,539,457\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVWxjozXQyl6"
      },
      "source": [
        "# Import Data\n",
        "All images are resized to (150x150). The training dataset is split into training & validation datasets. Most of the preprocessing is specified within the model (see above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhw3dducvMmr",
        "outputId": "1315ceca-c8ab-4c8b-bb83-40a74dde0efd"
      },
      "source": [
        "import tensorflow.keras.preprocessing as preprocessing\n",
        "from google.colab import drive\n",
        "\n",
        "print('Importing training and test datasets...\\n')\n",
        "batch_size = 64\n",
        "\n",
        "train_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/p3_data/cats_dogs_dataset/dataset/training_set'\n",
        "all_ds = preprocessing.image_dataset_from_directory(train_path,\n",
        "                                                    image_size=(150,150),\n",
        "                                                    shuffle=True,\n",
        "                                                    label_mode='binary',\n",
        "                                                    batch_size=batch_size)\n",
        "\n",
        "val_size = 12\n",
        "val_ds = all_ds.take(val_size)\n",
        "train_ds = all_ds.skip(val_size)\n",
        "\n",
        "test_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/p3_data/cats_dogs_dataset/dataset/test_set'\n",
        "test_ds = preprocessing.image_dataset_from_directory(test_path,\n",
        "                                                     image_size=(150,150),\n",
        "                                                     label_mode='binary',\n",
        "                                                     batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing training and test datasets...\n",
            "\n",
            "Found 8282 files belonging to 2 classes.\n",
            "Found 2003 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpOLl6TbEeAY"
      },
      "source": [
        "# Train\n",
        "The model is compiled with RMSprop optimizer and binary cross-entropy loss function.\n",
        "\n",
        "Training continues for 3 epochs, with 0.2% of the training set used for validation to check generalization.\n",
        "\n",
        "Weights are saved periodically during training to the \"training\" directory. When model fitting is complete, the model is saved to \"last_trained\" directory for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN5eMw5EPVzW"
      },
      "source": [
        "# Load pre-trained model and SKIP NEXT CODE BLOCK\n",
        "model = keras.models.load_model(r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/saved_models/v4_catdog_binaryaccuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1cahjH-_POA",
        "outputId": "b05cc031-0f35-4224-96af-789f61829a36"
      },
      "source": [
        "# fit model & save weights during training & after\n",
        "checkpoint_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "\n",
        "print('Fitting to data...\\n')\n",
        "history = model.fit(train_ds,\n",
        "                    epochs=5,\n",
        "                    callbacks=[cp_callback],\n",
        "                    validation_data=val_ds)\n",
        "\n",
        "save_path = r'/content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/last_trained'\n",
        "model.save(save_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting to data...\n",
            "\n",
            "Epoch 1/5\n",
            "118/118 [==============================] - 2117s 16s/step - loss: 4.1973 - accuracy: 0.8630 - val_loss: 0.7033 - val_accuracy: 0.8906\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 2/5\n",
            "118/118 [==============================] - 789s 7s/step - loss: 0.4829 - accuracy: 0.9201 - val_loss: 0.1295 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 3/5\n",
            "118/118 [==============================] - 803s 7s/step - loss: 0.2926 - accuracy: 0.9261 - val_loss: 0.3545 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 4/5\n",
            "118/118 [==============================] - 813s 7s/step - loss: 0.2330 - accuracy: 0.9359 - val_loss: 0.2514 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "Epoch 5/5\n",
            "118/118 [==============================] - 789s 7s/step - loss: 0.2330 - accuracy: 0.9362 - val_loss: 0.2336 - val_accuracy: 0.9622\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/training\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Spring2021/Deep Learning & Computer Vision/last_trained/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OayS_KR9tid4"
      },
      "source": [
        "# get sample from training set for sanity check\n",
        "def get_preds_labels(dat):\n",
        "  truths = []\n",
        "  preds = []\n",
        "  for X,Y in dat:\n",
        "    truths.append(Y.numpy().astype(int))\n",
        "    preds.append(model.predict(X))\n",
        "\n",
        "  preds = np.concatenate(preds, axis=0)\n",
        "  truths = np.concatenate(truths, axis=0)\n",
        "  print(preds.shape, truths.shape)\n",
        "  # Threshold at 0.9\n",
        "  preds = np.where(preds>0.9,1,0).astype(int)\n",
        "\n",
        "  return (preds, truths)\n",
        "\n",
        "print('Evaluating against test set...')\n",
        "results = model.evaluate(test_ds)\n",
        "\n",
        "print('Running predictions on test set...')\n",
        "preds, truths = get_preds_labels(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "6HN8T0cVPV0a",
        "outputId": "8ce5a267-ea7b-4179-e547-4ba2a036c5bb"
      },
      "source": [
        "def show_conf_mat(y_pred, y_true, class_names):\n",
        "  con_mat = confusion_matrix(labels=y_true, predictions=y_pred).numpy()\n",
        "  \n",
        "  fig, ax = plt.subplots()\n",
        "  ax.matshow(con_mat, cmap='cividis')\n",
        "  for (i, j), z in np.ndenumerate(con_mat):\n",
        "    ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n",
        "  ax.set_xticklabels(['']+class_names)\n",
        "  ax.set_yticklabels(['']+class_names)\n",
        "  plt.show()\n",
        "\n",
        "show_conf_mat(preds.reshape(-1), truths.reshape(-1), test_ds.class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}